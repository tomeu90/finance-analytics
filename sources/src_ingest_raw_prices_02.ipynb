{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1fc7821-a1a8-4067-8e2f-c2946ba46e81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install yfinance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0083a6fa-0bb6-4556-86d1-25307e498785",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "afe6cf2e-7d90-4063-94b8-6fcf7a61501d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import re\n",
    "import time\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3464d1a-1c09-474d-94b3-5999f33f0d61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "TICKER_TABLE = \"finance_catalog.db_landing.src_raw_index_keys\"\n",
    "TABLE_NAME = \"finance_catalog.db_landing.src_raw_stock_prices\"\n",
    "DEFAULT_LOOKBACK_DAYS = 365\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b8d995c-596e-4eaa-aeba-49be7ce859eb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "tickers = (\n",
    "    spark\n",
    "        .table(TICKER_TABLE)\n",
    "        .select(regexp_replace(\"ticker\", \"\\\\.\", \"-\").alias(\"ticker\"))\n",
    "        .toPandas()[\"ticker\"]\n",
    "        .tolist()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c147af28-a99e-4a16-bb58-a3550eb968da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def chunk_list(lst, size):\n",
    "    for i in range(0, len(lst), size):\n",
    "        yield lst[i:i + size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bf5a2804-9d8c-44fc-94ed-1848a2819123",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "ticker_data = []\n",
    "\n",
    "for batch in chunk_list(tickers[5000:], BATCH_SIZE):\n",
    "    for ticker in batch:\n",
    " \n",
    "        max_date_row = spark.sql(f\"SELECT MAX(date) as max_date FROM {TABLE_NAME} WHERE ticker = '{ticker}' GROUP BY ticker\").collect()\n",
    "        max_date = max_date_row[0]['max_date'] if max_date_row else None\n",
    "        start_date = max_date if max_date else datetime.now() - timedelta(days=DEFAULT_LOOKBACK_DAYS)\n",
    "\n",
    "        df = yf.download(ticker, period='1d', start=start_date, end=datetime.now() + timedelta(days=1), auto_adjust=True)\n",
    "        df = df.reset_index()\n",
    "        df.columns = df.columns.droplevel(1)\n",
    "        df.columns = [re.sub(r\"\\W+\", \"_\", col).strip(\"_\").lower() for col in df.columns]\n",
    "        df['ticker'] = ticker\n",
    "        df['id'] = ticker + df['date'].dt.strftime('%Y%m%d')\n",
    "        df.set_index('id', inplace=True)\n",
    "        df.reset_index(inplace=True)\n",
    "        ticker_data.append(df)\n",
    "\n",
    "        time.sleep(1)     \n",
    "\n",
    "    if ticker_data:\n",
    "        batch_df = pd.concat(ticker_data, ignore_index=True)\n",
    "        ticker_data.clear()\n",
    "\n",
    "    batch_df['volume'] = batch_df['volume'].astype(float)\n",
    "\n",
    "    (\n",
    "        spark\n",
    "            .createDataFrame(batch_df)\n",
    "            .orderBy([\"ticker\", \"date\"], ascending=[True, False])\n",
    "            .write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"append\")\n",
    "            .saveAsTable(TABLE_NAME)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9001d9aa-5022-45fb-b558-fba82162062d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ticker_data = []\n",
    "\n",
    "# for ticker in tickers[5000:]:\n",
    "#     max_date_row = spark.sql(f\"SELECT MAX(date) as max_date FROM {TABLE_NAME} WHERE ticker = '{ticker}' GROUP BY ticker\").collect()\n",
    "#     max_date = max_date_row[0]['max_date'] if max_date_row else None\n",
    "\n",
    "#     if max_date:\n",
    "#         start_date = max_date\n",
    "#     else:\n",
    "#         start_date = datetime.now() - timedelta(days=DEFAULT_LOOKBACK_DAYS)\n",
    "\n",
    "#     df = yf.download(ticker, period='1d', start=start_date, end=datetime.now() + timedelta(days=1), auto_adjust=True)\n",
    "#     df = df.reset_index()\n",
    "#     df.columns = df.columns.droplevel(1)\n",
    "#     df.columns = [re.sub(r\"\\W+\", \"_\", col).strip(\"_\").lower() for col in df.columns]\n",
    "#     df['ticker'] = ticker\n",
    "#     df['id'] = ticker + df['date'].dt.strftime('%Y%m%d')\n",
    "#     df.set_index('id', inplace=True)\n",
    "#     df.reset_index(inplace=True)\n",
    "#     ticker_data.append(df)\n",
    "\n",
    "# if ticker_data:\n",
    "#     df = pd.concat(ticker_data, ignore_index=True)\n",
    "#     df['volume'] = df['volume'].astype(float)\n",
    "                            \n",
    "# (\n",
    "#     spark\n",
    "#         .createDataFrame(df)\n",
    "#         .orderBy([\"ticker\", \"date\"], ascending=[True, False])\n",
    "#         .write\n",
    "#         .format(\"delta\")\n",
    "#         .mode(\"append\")\n",
    "#         .saveAsTable(TABLE_NAME)\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bd3bb086-65dc-455b-82ac-4606be1dfa92",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(\n",
    "    spark\n",
    "        .table(TABLE_NAME)\n",
    "        .dropDuplicates(subset=['ticker', 'date'])\n",
    "        .orderBy(['ticker', 'date'], ascending=[True, False])\n",
    "        .write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")\n",
    "        .saveAsTable(TABLE_NAME)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f373d7ae-29ee-439b-ba9f-498223fcf29d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.table(TABLE_NAME).write.mode(\"overwrite\").parquet(STORAGE_URL) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0c89077b-95e1-45cd-bdb5-c67cae269cc5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# spark.table(TABLE_NAME).display()\n",
    "# spark.sql(f\"DROP TABLE IF EXISTS {TABLE_NAME}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "src_ingest_raw_prices_02",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
